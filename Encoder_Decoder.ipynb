{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PURPOSE:  \n",
    "This program implements a embedded neural network in tensorflow to perform an encoder/decoder anomaly detector for different loan types.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INPUT: \n",
    "Loan data by type and features developed in the prior programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT: \n",
    "Anomaly detection on a known data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard python and sklearn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os as os\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tensorflow and tensorflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,callbacks,losses,optimizers,initializers,models,regularizers\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,Embedding,Flatten,concatenate,Input\n",
    "from tensorflow.keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.optimizers import SGD,RMSprop,Adam,Adamax\n",
    "from tensorflow.keras.initializers import RandomNormal,RandomUniform,TruncatedNormal,Glorot_Normal,Normal\n",
    "from tensorflow.keras.metrics import mae, mse, mape\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import mae, mse, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed for initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mset_random_seed(\u001b[38;5;241m79\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for gpu and expect this output:\n",
    "\n",
    "[\n",
    "  name: \"/cpu:0\"device_type: \"CPU\",\n",
    "  name: \"/gpu:0\"device_type: \"GPU\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12355073655885624654\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13582938859459649461\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load(os.getcwd() + 'loan_data')\n",
    "df.sort_values(by=['',''],inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert dates to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dates to categorical variables - retain raw dataframe df\n",
    "def add_date_features(data,date,name):\n",
    "    data[name + 'Yr'] = data[date].dt.year\n",
    "    #data[name + 'Day'] = data[date].dt.dayofyear\n",
    "    #data[name + 'Week'] = data[date].dt.week\n",
    "    #data[name + 'Mon'] = data[date].dt.month \n",
    "    data[name + 'Qtr'] = data[date].dt.quarter\n",
    "    data.drop([date], axis = 1, inplace = True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form date features and drop datetime entry\n",
    "dates = ['','']\n",
    "for d in dates:\n",
    "    dt = add_date_features[df,date,name] # date is column name and name is new column name\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify categorical, continuous, and time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "dt.columns\n",
    "\n",
    "# Choose Categorical Feature Vars\n",
    "cat_vars = ['', '', '']\n",
    "\n",
    "# Choose Time Features Vars\n",
    "time_vars = ['', '']\n",
    "\n",
    "# # Choose Continuous Feature Vars To Be Scaled\n",
    "cont_vars = ['','']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create categorical input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode(dt,cat_vars):\n",
    "    cat_transformer = ColumnTransformer(\n",
    "    [(\"cat_encoder\", LabelEncoder(), cat_vars)]\n",
    "    dt = cat_transformer.fit_transform(dt)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_embed(dt, cat_vars, emax=6, emin=3):\n",
    "    cat_class = [len(df[c].unique()) for c in cat_vars]\n",
    "    cat_class_dict = dict(zip(cat_vars,cat_class))\n",
    "    cat_emb = [emax if x > emax else x for x in cat_class]\n",
    "    cat_emb = [emin if x < emin else x for x in cat_emb]\n",
    "    cat_emb_dict = dict(zip(cat_vars,cat_emb))\n",
    "    return cat_class_dict, cat_vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_input(var,cat_vars_dict,cat_class_dict,r=.2):\n",
    "    name = var\n",
    "    c1 = cat_class_dict[name]\n",
    "    c2 = cat_vars_dict[name]\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    #embedding layer is map of number of classes (c1) to number of embedded features (c2)\n",
    "    ct = Flatten(name=name+'_flt')(Embedding(c1,c2, embeddings_initializer='glorot_normal')(inp))\n",
    "     # add dense layers and dropout\n",
    "    ct = Dense(128, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(ct)\n",
    "    ct = Dropout(rate=r)(ct)\n",
    "    return inp,ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_encode(dt,time_vars):\n",
    "    time_transformer = ColumnTransformer(\n",
    "    [(\"time_encoder\", LabelEncoder(), time_vars)]\n",
    "    dt = time_transformer.fit_transform(dt)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_embed(dt, time_vars, emax=6, emin=3):\n",
    "    time_class = [len(df[t].unique()) for t in time_vars]\n",
    "    time_class_dict = dict(zip(time_vars,time_class))\n",
    "    time_emb = [emax if x > emax else x for x in time_class]\n",
    "    time_emb = [emin if x <= emin else x for x in time_emb]\n",
    "    time_emb_dict = dict(zip(time_vars,time_emb))\n",
    "    return time_class_dict, time_vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_input(var,time_vars_dict,time_class_dict,r=.2):\n",
    "    name = var\n",
    "    c1 = time_class_dict[name]\n",
    "    c2 = time_vars_dict[name]\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    #embedding layer is map of number of classes (c1) to number of embedded features (c2)\n",
    "    t = Flatten(name=name+'_flt')(Embedding(c1,c2,embeddings_initializer='glorot_normal')(inp))\n",
    "    t = Dense(128, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(t)\n",
    "    t = Dropout(rate=r)(t)\n",
    "    return inp,t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create continuous input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "# s can be standardscaler,robustscaler or minmaxscaler; default is minmax\n",
    "# x,y is limit on minmax; default to 0,1\n",
    "# l,u is percential rank for the robust scaler based on median; default is 10,90\n",
    "\n",
    "def cont_scaler(dt, var, scaler='minmax', x=1, y=5, l=10, u=90): # s can be standardscaler,robustscaler or minmaxscaler\n",
    "    # select scaler map and form list of tuples for variable and scaler\n",
    "  if scaler == 'standard':\n",
    "      var_scaled = StandardScaler().fit_transform(dt[var])\n",
    "\n",
    "  elif scaler == 'robust':\n",
    "      var_scaled = RobustScaler(with_centering=True,with_scaling=True,quantile_range=(l,u)).fit_transform(dt[var])\n",
    "\n",
    "  elif scaler == 'minmax':\n",
    "      var_scaled = MinMaxScaler(feature_range = (x,y)).fit_transform(dt[var])\n",
    "\n",
    "  # return map of scaler and continuous variables tuples\n",
    "  return cont_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaler Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_scale_var(dt, var, scaler):  #scaler can be 'standard','robust','minmax', var is a 'column name' \n",
    "    dt[var] = cont_scaler(var, scaler)\n",
    "    return dt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize and scale continuous variables to relieve skew and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_scale(dt,cont_vars):\n",
    "    for var in cont_vars:\n",
    "        dt = cont_scale_var(dt, var, 'standard') #normalize distribution on mean = 0\n",
    "        dt = cont_scale_var(dt, var, 'minmax')  #scale distribution to positive range\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_input(var):\n",
    "    name = var\n",
    "    inp = Input((1,), name=name+'_in')\n",
    "    d = Dense(1, name = name + '_d')(inp)\n",
    "    d = Dense(128, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(d)\n",
    "    d = Dropout(rate=r)(d)\n",
    "    return inp,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set train, test, validate sets with validation as one quarter of each year and test at last quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df,vstart=2,tstart=1):\n",
    "    dates = list(df.Date.unique())\n",
    "    dates.sort()\n",
    "    dates_validate = dates[-vstart:]\n",
    "    #dates_test = dates[-tstart:]\n",
    "    dates_train = dates[:-vstart]\n",
    "    data = df.sort_values(by=['ID','Date'])\n",
    "    data_train = data.loc[data.Date.isin(dates_train)]\n",
    "    data_validate = data.loc[data.Date.isin(dates_validate)]\n",
    "    return data_train,data_validate,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train,data_validate,data=split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and scale data and reshape into array of vectors. \n",
    "___________________________________________________________________________________________________________\n",
    "Since the input layer of the neural network is a horizontally concatenated layer of each categorical variable in its own embedding input shared with the continuous variables each in its own dense input the train, validate and test data needs to be reshaped into a list of vectors for each feature.  To keep the array in mixed dtypes (i.e., int and float), input data is a list of arrays with each element in the list being a vector for the shared input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_shape_data(data_train,data_validate,cat_map_fit,cont_map_fit):\n",
    "    #set target variables\n",
    "    y_tr = data_train.REV.values.reshape(-1,1)\n",
    "    y_val = data_validate.REV.values.reshape(-1,1)\n",
    "    #transform categorical data\n",
    "    cat_train = cat_map_fit.transform(data_train).astype(np.int64)\n",
    "    cat_validate = cat_map_fit.transform(data_validate).astype(np.int64)\n",
    "    #transform continuous variables\n",
    "    cont_train = cont_map_fit.transform(data_train).astype(np.float32)\n",
    "    cont_validate = cont_map_fit.transform(data_validate).astype(np.float32)\n",
    "    #combine categorical and continuous data into array of vectors\n",
    "    data_tr = np.hsplit(cat_train,cat_train.shape[1])+np.hsplit(cont_train,cont_train.shape[1])\n",
    "    data_val = np.hsplit(cat_validate,cat_validate.shape[1])+np.hsplit(cont_validate,cont_validate.shape[1])\n",
    "    return y_tr,y_val,data_tr,data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr,y_val,data_tr,data_val = map_shape_data(data_train,data_validate,cat_map_fit,cont_map_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create single input vector (input_shape = 1) for categorical input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_input(feat,cat_vars_dict):\n",
    "    name = feat[0]\n",
    "    c1 = len(feat[1].classes_)\n",
    "    c2 = cat_vars_dict[name]\n",
    "    if c2 > 50:c2 = 50\n",
    "    if c2 < 5:c2 = 5\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    #no third dimension for a time distributed series so flattened into column of 1\n",
    "    #embedding layer is map of number of classes (c) to number of embedded features (c2)\n",
    "    u = Flatten(name=name+'_flt')(Embedding(c1,c2,input_length=1)(inp))\n",
    "    return inp,u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of Input,Flatten,and Embedding layers for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = [cat_input(feat,cat_vars_dict) for feat in cat_map_fit.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deprecation warning is an incompatibility between keras and tensorflow.keras.  The error message is an outstanding bug in tensorflow and does not occur in keras.  Tensorflow has an open issue report regarding this error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create Input and Dense layer for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_input(feat):\n",
    "    name = feat[0][0]\n",
    "    inp = Input((1,), name=name+'_in')\n",
    "    d = Dense(1, name = name + '_d')(inp)\n",
    "    return inp,d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of Input and Dense layers for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = [cont_input(feat) for feat in cont_map_fit.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a four layer model using a shared input layer for the categorical and continuous variables.  The hideen 2 layers are high node counts because sample count in input data is large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model(conts,embs):\n",
    "    #concatenate the inputs and embedded layers with the inputs and continuous dense layers\n",
    "    #referred to as 'shared layers' in tensorflow.keras documentation\n",
    "    x = concatenate([emb for inp,emb in embs] + [d for inp,d in conts])\n",
    "    #apply L2 normalization using the BatchNormalization method on continuous features\n",
    "    x = Dense(128, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    #apply small dropout for first normalization\n",
    "    x = Dropout(rate=0.6)(x)\n",
    "    #apply additional L2 normalization using the BatchNormalization method\n",
    "    x =\tBatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    #apply small dropout for normalization\n",
    "    x =\tDropout(rate=0.6)(x)\n",
    "    #apply L2 normalization using the BatchNormalization method\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64,activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)  \n",
    "    x =\tDropout(rate=0.6)(x)\n",
    "    #apply L2 normalization using the BatchNormalization method\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    model = Model([inp for inp,emb in embs] + [inp for inp,d in conts], x)\n",
    "    model.compile(optimizer='Adam',loss='mean_absolute_error',metrics=['mape'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement logger,reduce the learning rate when loss function change gets small,add early stopping and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('Partner_Error.csv')\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=5,min_lr=0.0001)\n",
    "mc = ModelCheckpoint('Partner_Best_Model',save_best_only=True)\n",
    "model = embed_model(conts,embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next process is cpu/gpu intensive.  This code should be run on a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/25\n",
      "7000/7000 [==============================] - 14s 2ms/sample - loss: 0.2934 - mean_absolute_percentage_error: 566299.8750 - val_loss: 0.3155 - val_mean_absolute_percentage_error: 37.5394\n",
      "Epoch 2/25\n",
      "7000/7000 [==============================] - 2s 240us/sample - loss: 0.1443 - mean_absolute_percentage_error: 668365.8125 - val_loss: 0.2060 - val_mean_absolute_percentage_error: 24.9947\n",
      "Epoch 3/25\n",
      "7000/7000 [==============================] - 2s 241us/sample - loss: 0.1142 - mean_absolute_percentage_error: 732329.5000 - val_loss: 0.1040 - val_mean_absolute_percentage_error: 13.3476\n",
      "Epoch 4/25\n",
      "7000/7000 [==============================] - 2s 245us/sample - loss: 0.1017 - mean_absolute_percentage_error: 645652.4375 - val_loss: 0.0941 - val_mean_absolute_percentage_error: 13.3687\n",
      "Epoch 5/25\n",
      "7000/7000 [==============================] - 2s 241us/sample - loss: 0.0993 - mean_absolute_percentage_error: 692265.1250 - val_loss: 0.0774 - val_mean_absolute_percentage_error: 12.5370\n",
      "Epoch 6/25\n",
      "7000/7000 [==============================] - 1s 209us/sample - loss: 0.0938 - mean_absolute_percentage_error: 602611.3750 - val_loss: 0.0813 - val_mean_absolute_percentage_error: 13.1877\n",
      "Epoch 7/25\n",
      "7000/7000 [==============================] - 2s 253us/sample - loss: 0.0912 - mean_absolute_percentage_error: 609908.1875 - val_loss: 0.0678 - val_mean_absolute_percentage_error: 11.2277\n",
      "Epoch 8/25\n",
      "7000/7000 [==============================] - 2s 238us/sample - loss: 0.0888 - mean_absolute_percentage_error: 286932.2188 - val_loss: 0.0873 - val_mean_absolute_percentage_error: 14.0984\n",
      "Epoch 9/25\n",
      "7000/7000 [==============================] - 2s 236us/sample - loss: 0.0872 - mean_absolute_percentage_error: 536091.1250 - val_loss: 0.1042 - val_mean_absolute_percentage_error: 16.3452\n",
      "Epoch 10/25\n",
      "7000/7000 [==============================] - 2s 232us/sample - loss: 0.0831 - mean_absolute_percentage_error: 351367.2812 - val_loss: 0.1109 - val_mean_absolute_percentage_error: 16.4219\n",
      "Epoch 11/25\n",
      "7000/7000 [==============================] - 2s 253us/sample - loss: 0.0828 - mean_absolute_percentage_error: 440404.7812 - val_loss: 0.1609 - val_mean_absolute_percentage_error: 22.1761\n",
      "Epoch 12/25\n",
      "7000/7000 [==============================] - 2s 316us/sample - loss: 0.0838 - mean_absolute_percentage_error: 494173.4688 - val_loss: 0.0977 - val_mean_absolute_percentage_error: 14.9560\n",
      "Epoch 13/25\n",
      "7000/7000 [==============================] - 2s 228us/sample - loss: 0.0798 - mean_absolute_percentage_error: 668625.7500 - val_loss: 0.0931 - val_mean_absolute_percentage_error: 14.2550\n",
      "Epoch 14/25\n",
      "7000/7000 [==============================] - 2s 246us/sample - loss: 0.0805 - mean_absolute_percentage_error: 475041.3125 - val_loss: 0.0817 - val_mean_absolute_percentage_error: 12.7421\n",
      "Epoch 15/25\n",
      "7000/7000 [==============================] - 2s 221us/sample - loss: 0.0799 - mean_absolute_percentage_error: 785916.0000 - val_loss: 0.0810 - val_mean_absolute_percentage_error: 12.7984\n",
      "Epoch 16/25\n",
      "7000/7000 [==============================] - 2s 227us/sample - loss: 0.0784 - mean_absolute_percentage_error: 471112.1562 - val_loss: 0.0875 - val_mean_absolute_percentage_error: 13.5448\n",
      "Epoch 17/25\n",
      "7000/7000 [==============================] - 2s 230us/sample - loss: 0.0791 - mean_absolute_percentage_error: 722439.3750 - val_loss: 0.0834 - val_mean_absolute_percentage_error: 12.9600\n",
      "Epoch 18/25\n",
      "7000/7000 [==============================] - 2s 227us/sample - loss: 0.0778 - mean_absolute_percentage_error: 594458.9375 - val_loss: 0.0860 - val_mean_absolute_percentage_error: 13.2547\n",
      "Epoch 19/25\n",
      "7000/7000 [==============================] - 2s 260us/sample - loss: 0.0792 - mean_absolute_percentage_error: 464562.9688 - val_loss: 0.0881 - val_mean_absolute_percentage_error: 13.6288\n",
      "Epoch 20/25\n",
      "7000/7000 [==============================] - 2s 242us/sample - loss: 0.0773 - mean_absolute_percentage_error: 479893.2812 - val_loss: 0.0852 - val_mean_absolute_percentage_error: 13.2478\n",
      "Epoch 21/25\n",
      "7000/7000 [==============================] - 2s 223us/sample - loss: 0.0809 - mean_absolute_percentage_error: 692652.5000 - val_loss: 0.0934 - val_mean_absolute_percentage_error: 14.3821\n",
      "Epoch 22/25\n",
      "7000/7000 [==============================] - 2s 227us/sample - loss: 0.0770 - mean_absolute_percentage_error: 687335.3750 - val_loss: 0.0979 - val_mean_absolute_percentage_error: 14.8650\n",
      "Epoch 23/25\n",
      "7000/7000 [==============================] - 2s 230us/sample - loss: 0.0788 - mean_absolute_percentage_error: 725083.5625 - val_loss: 0.0844 - val_mean_absolute_percentage_error: 13.3129\n",
      "Epoch 24/25\n",
      "7000/7000 [==============================] - 2s 230us/sample - loss: 0.0761 - mean_absolute_percentage_error: 409810.8125 - val_loss: 0.0871 - val_mean_absolute_percentage_error: 13.5906\n",
      "Epoch 25/25\n",
      "7000/7000 [==============================] - 2s 220us/sample - loss: 0.0787 - mean_absolute_percentage_error: 437258.9375 - val_loss: 0.0906 - val_mean_absolute_percentage_error: 13.7485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9541b062b0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_tr,y_tr,batch_size=64,epochs=25,verbose=1,validation_data = (data_val,y_val),callbacks=[csv_logger,rlr,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Partner_Best_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model_data,model=model):\n",
    "    pred = model.predict(model_data)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = prediction(data_tr)\n",
    "pred_val = prediction(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_list(arr):\n",
    "    listed = [item for sublist in arr for item in sublist]\n",
    "    return listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_dataframe(df,pred_tr,pred_val,y_tr,y_val):\n",
    "    pred_tr = array_to_list(pred_tr)\n",
    "    pred_val = array_to_list(pred_val)\n",
    "    preds = pred_tr + pred_val\n",
    "    actuals = list(y_tr) + list(y_val)\n",
    "    dr = pd.DataFrame()\n",
    "    dr['Date'] = df.Date\n",
    "    dr['ID'] = df.ID\n",
    "    dr['Actual'] = actuals\n",
    "    dr['Predict'] = preds\n",
    "    dr = dr.loc[:,['ID','Date','Actual','Predict']]\n",
    "    dr.to_pickle('Scaled_Predictions_Qtr.pkl')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_dataframe(df,pred_tr,pred_val,y_tr,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of code: Close this file using File 'Close and Halt' from dropdown menu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
